{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bo/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "100%|██████████| 588/588 [00:38<00:00, 15.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from util_get_seq_emb_data import get_seq_emb_dataset\n",
    "\n",
    "all_seqs = get_seq_emb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "input_size = 8\n",
    "hidden_size = 100\n",
    "latent_size = 20\n",
    "num_layers = 1\n",
    "num_epochs = 300\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MySequenceDataset(Dataset):\n",
    "    def __init__(self, data, stop_token):\n",
    "        \"\"\"\n",
    "        data: list of sequences; each sequence is a list of (1, 8)-shaped tensors\n",
    "        stop_token: a (1, 8)-shaped tensor to append\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.stop_token = stop_token\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq_list = self.data[idx]\n",
    "        # seq_list: list of (1, 8) tensors\n",
    "        seq_tensor = torch.cat(seq_list, dim=0)  # shape: (seq_len, 8)\n",
    "        # Add stop token\n",
    "        seq_tensor = torch.cat([seq_tensor, self.stop_token], dim=0)  \n",
    "        # Now shape: (seq_len+1, 8)\n",
    "        return seq_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list of tensors [seq1, seq2, ..., seqB]\n",
    "           where each seq is shape (seq_len_i, 8).\n",
    "    We want to pad them to the same length = max(seq_len_i).\n",
    "    \"\"\"\n",
    "    # 1) Find max length\n",
    "    max_len = max(seq.size(0) for seq in batch)\n",
    "    \n",
    "    # 2) Pad\n",
    "    padded_batch = []\n",
    "    for seq in batch:\n",
    "        seq_len = seq.size(0)\n",
    "        # shape: (seq_len, 8)\n",
    "        if seq_len < max_len:\n",
    "            pad_len = max_len - seq_len\n",
    "            pad_tensor = torch.zeros(pad_len, 8).cuda()\n",
    "            seq = torch.cat([seq, pad_tensor], dim=0)  # (max_len, 8)\n",
    "        padded_batch.append(seq.unsqueeze(0))  # (1, max_len, 8)\n",
    "    \n",
    "    # 3) Stack => (B, T, 8)\n",
    "    padded_batch = torch.cat(padded_batch, dim=0)\n",
    "    return padded_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "STOP_TOKEN = torch.zeros(1, 8).cuda()  # shape = (1, 8), all zeros\n",
    "\n",
    "dataset = MySequenceDataset(all_seqs, STOP_TOKEN)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size   = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, \n",
    "    lengths=[train_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class GRUVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,   # dimension of each embedding at a timestep\n",
    "        hidden_size: int,  # hidden dimension of GRU\n",
    "        latent_size: int,  # dimension of latent space z\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super(GRUVAE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.hidden2logvar = nn.Linear(hidden_size, latent_size)\n",
    "        \n",
    "        self.latent2hidden = nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.h2output = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # We only need the final hidden state from the GRU\n",
    "        _, h_n = self.encoder_gru(x)\n",
    "        \n",
    "        # h_n: (num_layers, batch_size, hidden_size)\n",
    "        # Let's take only the top layer\n",
    "        h_n_top = h_n[-1]  # shape: (batch_size, hidden_size)\n",
    "        \n",
    "        mu = self.hidden2mu(h_n_top)\n",
    "        logvar = self.hidden2logvar(h_n_top)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        Decodes a latent vector z into a sequence of length seq_len.\n",
    "        Args:\n",
    "            z: (batch_size, latent_size)\n",
    "            seq_len: int, the length of the output sequence\n",
    "        Returns:\n",
    "            outputs: (batch_size, seq_len, input_size)\n",
    "        \"\"\"\n",
    "        # Transform latent vector to initial hidden state for GRU\n",
    "        hidden = self.latent2hidden(z)         # (batch_size, hidden_size)\n",
    "        hidden = hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)  \n",
    "        \n",
    "        # We'll generate the sequence step by step.\n",
    "        batch_size = z.size(0)\n",
    "        outputs = []\n",
    "        \n",
    "        # Start with a zero vector as the \"input\" for each timestep\n",
    "        input_step = torch.zeros(batch_size, 1, self.h2output.out_features, device=z.device)\n",
    "        for t in range(seq_len):\n",
    "            # Pass one step at a time\n",
    "            out, hidden = self.decoder_gru(input_step, hidden)\n",
    "            # out: (batch_size, 1, hidden_size)\n",
    "            # Project back to embedding dimension\n",
    "            step_output = self.h2output(out)   # (batch_size, 1, input_size)\n",
    "            outputs.append(step_output)\n",
    "            \n",
    "            # The next input is the current output (autoregressive decoding)\n",
    "            input_step = step_output\n",
    "\n",
    "        # Concatenate along seq_len dimension\n",
    "        outputs = torch.cat(outputs, dim=1)    # (batch_size, seq_len, input_size)\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        seq_len = x.size(1)\n",
    "        recon_x = self.decode(z, seq_len)\n",
    "        return recon_x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss = Reconstruction Loss + KL Divergence.\n",
    "    Here we use MSE for reconstruction (for continuous data).\n",
    "    \"\"\"\n",
    "    # MSE Reconstruction Loss\n",
    "    mse_loss = nn.MSELoss(reduction='mean')\n",
    "    recon_loss = mse_loss(recon_x, x)\n",
    "\n",
    "    # KL Divergence\n",
    "    # D_KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + 0.0001*kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer\n",
    "model = GRUVAE(input_size, hidden_size, latent_size, num_layers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 0.2746 | Val Loss: 0.3769\n",
      "Epoch [2/300] | Train Loss: 0.2703 | Val Loss: 0.4035\n",
      "Epoch [3/300] | Train Loss: 0.2805 | Val Loss: 0.3811\n",
      "Epoch [4/300] | Train Loss: 0.2614 | Val Loss: 0.3780\n",
      "Epoch [5/300] | Train Loss: 0.2869 | Val Loss: 0.4030\n",
      "Epoch [6/300] | Train Loss: 0.2783 | Val Loss: 0.3738\n",
      "Epoch [7/300] | Train Loss: 0.2666 | Val Loss: 0.3826\n",
      "Epoch [8/300] | Train Loss: 0.2553 | Val Loss: 0.3687\n",
      "Epoch [9/300] | Train Loss: 0.2416 | Val Loss: 0.3784\n",
      "Epoch [10/300] | Train Loss: 0.2644 | Val Loss: 0.3693\n",
      "Epoch [11/300] | Train Loss: 0.2466 | Val Loss: 0.3706\n",
      "Epoch [12/300] | Train Loss: 0.2430 | Val Loss: 0.3845\n",
      "Epoch [13/300] | Train Loss: 0.2829 | Val Loss: 0.3606\n",
      "Epoch [14/300] | Train Loss: 0.2891 | Val Loss: 0.3879\n",
      "Epoch [15/300] | Train Loss: 0.2628 | Val Loss: 0.3598\n",
      "Epoch [16/300] | Train Loss: 0.2464 | Val Loss: 0.3589\n",
      "Epoch [17/300] | Train Loss: 0.2482 | Val Loss: 0.3530\n",
      "Epoch [18/300] | Train Loss: 0.2824 | Val Loss: 0.3901\n",
      "Epoch [19/300] | Train Loss: 0.2540 | Val Loss: 0.3597\n",
      "Epoch [20/300] | Train Loss: 0.2419 | Val Loss: 0.3653\n",
      "Epoch [21/300] | Train Loss: 0.2383 | Val Loss: 0.3538\n",
      "Epoch [22/300] | Train Loss: 0.2487 | Val Loss: 0.3588\n",
      "Epoch [23/300] | Train Loss: 0.2455 | Val Loss: 0.3500\n",
      "Epoch [24/300] | Train Loss: 0.2545 | Val Loss: 0.3782\n",
      "Epoch [25/300] | Train Loss: 0.2446 | Val Loss: 0.3559\n",
      "Epoch [26/300] | Train Loss: 0.2373 | Val Loss: 0.3399\n",
      "Epoch [27/300] | Train Loss: 0.2390 | Val Loss: 0.3450\n",
      "Epoch [28/300] | Train Loss: 0.2363 | Val Loss: 0.3526\n",
      "Epoch [29/300] | Train Loss: 0.2310 | Val Loss: 0.3364\n",
      "Epoch [30/300] | Train Loss: 0.2359 | Val Loss: 0.3550\n",
      "Epoch [31/300] | Train Loss: 0.2281 | Val Loss: 0.3400\n",
      "Epoch [32/300] | Train Loss: 0.2536 | Val Loss: 0.3544\n",
      "Epoch [33/300] | Train Loss: 0.2551 | Val Loss: 0.3598\n",
      "Epoch [34/300] | Train Loss: 0.2448 | Val Loss: 0.3359\n",
      "Epoch [35/300] | Train Loss: 0.2394 | Val Loss: 0.3355\n",
      "Epoch [36/300] | Train Loss: 0.2411 | Val Loss: 0.3617\n",
      "Epoch [37/300] | Train Loss: 0.2298 | Val Loss: 0.3454\n",
      "Epoch [38/300] | Train Loss: 0.2347 | Val Loss: 0.3422\n",
      "Epoch [39/300] | Train Loss: 0.2388 | Val Loss: 0.3432\n",
      "Epoch [40/300] | Train Loss: 0.2383 | Val Loss: 0.3568\n",
      "Epoch [41/300] | Train Loss: 0.2459 | Val Loss: 0.3447\n",
      "Epoch [42/300] | Train Loss: 0.2326 | Val Loss: 0.3345\n",
      "Epoch [43/300] | Train Loss: 0.2257 | Val Loss: 0.3414\n",
      "Epoch [44/300] | Train Loss: 0.2201 | Val Loss: 0.3421\n",
      "Epoch [45/300] | Train Loss: 0.2302 | Val Loss: 0.3386\n",
      "Epoch [46/300] | Train Loss: 0.2236 | Val Loss: 0.3291\n",
      "Epoch [47/300] | Train Loss: 0.2233 | Val Loss: 0.3281\n",
      "Epoch [48/300] | Train Loss: 0.2214 | Val Loss: 0.3289\n",
      "Epoch [49/300] | Train Loss: 0.2214 | Val Loss: 0.3232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      4\u001b[0m     total_train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m batch_data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      7\u001b[0m         batch_data \u001b[39m=\u001b[39m batch_data\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      8\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices])  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mMySequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m seq_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx]\n\u001b[1;32m     17\u001b[0m \u001b[39m# seq_list: list of (1, 8) tensors\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m seq_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(seq_list, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# shape: (seq_len, 8)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Add stop token\u001b[39;00m\n\u001b[1;32m     20\u001b[0m seq_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([seq_tensor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_token], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    total_train_loss = 0.0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        batch_data = batch_data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_x, mu, logvar = model(batch_data)\n",
    "        \n",
    "        loss = loss_function(recon_x, batch_data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    \n",
    "    model.eval()         \n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # no gradient calc\n",
    "        for val_data in val_loader:\n",
    "            val_data = val_data.cuda()\n",
    "            \n",
    "            recon_x, mu, logvar = model(val_data)\n",
    "            val_loss = loss_function(recon_x, val_data, mu, logvar)\n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72948"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"pt_model.pth\"\n",
    "\n",
    "# Option 1: Save only the model's state_dict\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7df1d8b499a47504649464ef14cc540ce0a3f157eaf8a1bd6897e9769f098f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

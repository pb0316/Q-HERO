{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "\n",
    "class GRUVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,   # dimension of each embedding at a timestep\n",
    "        hidden_size: int,  # hidden dimension of GRU\n",
    "        latent_size: int,  # dimension of latent space z\n",
    "        num_layers: int = 1,\n",
    "    ):\n",
    "        super(GRUVAE, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        self.encoder_gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.hidden2mu = nn.Linear(hidden_size, latent_size)\n",
    "        self.hidden2logvar = nn.Linear(hidden_size, latent_size)\n",
    "        \n",
    "        self.latent2hidden = nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.h2output = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # We only need the final hidden state from the GRU\n",
    "        _, h_n = self.encoder_gru(x)\n",
    "        \n",
    "        # h_n: (num_layers, batch_size, hidden_size)\n",
    "        # Let's take only the top layer\n",
    "        h_n_top = h_n[-1]  # shape: (batch_size, hidden_size)\n",
    "        \n",
    "        mu = self.hidden2mu(h_n_top)\n",
    "        logvar = self.hidden2logvar(h_n_top)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, seq_len):\n",
    "        \"\"\"\n",
    "        Decodes a latent vector z into a sequence of length seq_len.\n",
    "        Args:\n",
    "            z: (batch_size, latent_size)\n",
    "            seq_len: int, the length of the output sequence\n",
    "        Returns:\n",
    "            outputs: (batch_size, seq_len, input_size)\n",
    "        \"\"\"\n",
    "        # Transform latent vector to initial hidden state for GRU\n",
    "        hidden = self.latent2hidden(z)         # (batch_size, hidden_size)\n",
    "        hidden = hidden.unsqueeze(0).repeat(self.num_layers, 1, 1)  \n",
    "        \n",
    "        # We'll generate the sequence step by step.\n",
    "        batch_size = z.size(0)\n",
    "        outputs = []\n",
    "        \n",
    "        # Start with a zero vector as the \"input\" for each timestep\n",
    "        input_step = torch.zeros(batch_size, 1, self.h2output.out_features, device=z.device)\n",
    "        for t in range(seq_len):\n",
    "            # Pass one step at a time\n",
    "            out, hidden = self.decoder_gru(input_step, hidden)\n",
    "            # out: (batch_size, 1, hidden_size)\n",
    "            # Project back to embedding dimension\n",
    "            step_output = self.h2output(out)   # (batch_size, 1, input_size)\n",
    "            outputs.append(step_output)\n",
    "            \n",
    "            # The next input is the current output (autoregressive decoding)\n",
    "            input_step = step_output\n",
    "\n",
    "        # Concatenate along seq_len dimension\n",
    "        outputs = torch.cat(outputs, dim=1)    # (batch_size, seq_len, input_size)\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        seq_len = x.size(1)\n",
    "        recon_x = self.decode(z, seq_len)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "    def sample(self, batch_size=1, seq_len=10):\n",
    "\n",
    "        z = torch.randn(batch_size, self.latent_size).cuda()\n",
    "\n",
    "        # Decode to generate sequences\n",
    "        with torch.no_grad():\n",
    "            samples = self.decode(z, seq_len)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUVAE(\n",
       "  (encoder_gru): GRU(8, 100, batch_first=True)\n",
       "  (hidden2mu): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (hidden2logvar): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (latent2hidden): Linear(in_features=20, out_features=100, bias=True)\n",
       "  (decoder_gru): GRU(8, 100, batch_first=True)\n",
       "  (h2output): Linear(in_features=100, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GRUVAE(input_size=8, hidden_size=100, latent_size=20, num_layers=1).cuda()\n",
    "state_dict = torch.load('/home/bo/HierQAC/pt_model.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bo/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode back to molecule\n",
    "import sys\n",
    "sys.path.append(\"./hgraph2graph\")\n",
    "\n",
    "from hgraph2graph.hgraph import *\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from hgraph import HierVAE, common_atom_vocab\n",
    "from hgraph.hgnn import make_cuda\n",
    "from hgraph2graph.preprocess import tensorize\n",
    "from hgraph import PairVocab\n",
    "from util_split_core_tail import iterative_cut\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "#####################\n",
    "# Load Core Encoder #\n",
    "#####################\n",
    "\n",
    "core_vocab_path = \"hgraph2graph/vocab-cores.txt\"\n",
    "with open(core_vocab_path) as f:\n",
    "    core_vocab = [x.strip(\"\\r\\n \").split() for x in f]\n",
    "core_vocab = PairVocab(core_vocab, cuda=False)\n",
    "\n",
    "args_core = argparse.Namespace(\n",
    "    seed=7,\n",
    "    rnn_type='LSTM',\n",
    "    hidden_size=100,   \n",
    "    embed_size=100,    \n",
    "    latent_size=8,\n",
    "    depthT=15,\n",
    "    depthG=15,\n",
    "    diterT=1,\n",
    "    diterG=3,\n",
    "    dropout=0.0,\n",
    "    vocab=core_vocab,\n",
    "    atom_vocab=common_atom_vocab\n",
    ")\n",
    "\n",
    "core_model = HierVAE(args_core).cuda()\n",
    "core_model.load_state_dict(torch.load(\"hgraph2graph/ckpt/cores/model.ckpt.1000\")[0])\n",
    "\n",
    "#####################\n",
    "# Load Tail Encoder #\n",
    "#####################\n",
    "\n",
    "tail_vocab_path = \"hgraph2graph/vocab-tails.txt\"\n",
    "with open(tail_vocab_path) as f:\n",
    "    tail_vocab = [x.strip(\"\\r\\n \").split() for x in f]\n",
    "tail_vocab = PairVocab(tail_vocab, cuda=False)\n",
    "\n",
    "args_tail = argparse.Namespace(\n",
    "    seed=7,\n",
    "    rnn_type='LSTM',\n",
    "    hidden_size=100,   \n",
    "    embed_size=100,    \n",
    "    latent_size=8,\n",
    "    depthT=15,\n",
    "    depthG=15,\n",
    "    diterT=1,\n",
    "    diterG=3,\n",
    "    dropout=0.0,\n",
    "    vocab=tail_vocab,\n",
    "    atom_vocab=common_atom_vocab\n",
    ")\n",
    "\n",
    "tail_model = HierVAE(args_tail).cuda()\n",
    "tail_model.load_state_dict(torch.load(\"hgraph2graph/ckpt/tails/model.ckpt.800\")[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample one Z\n",
    "\n",
    "samples = model.sample(batch_size=1, seq_len=5)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3352620601654053\n"
     ]
    }
   ],
   "source": [
    "cores_z = samples[:,0,:]\n",
    "tails_z = samples[0,1:,:]\n",
    "\n",
    "core_mol = core_model.csample(cores_z, greedy=True)\n",
    "tail_mols = tail_model.csample(tails_z, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core: ['c1ccc(-c2cc[nH+]cc2)cc1']\n",
      "tails: ['CCCCCCCCCOC(=O)C[NH3+]', 'CCCCCC[NH3+]', 'CCCCC[NH3+]', 'CCCCC[NH3+]']\n",
      "0.0034019947052001953\n",
      "assembled: CCCCCCCCCOC(=O)C[n+]1ccc(-c2ccccc2)cc1\n"
     ]
    }
   ],
   "source": [
    "from util_reassemble_core_tail import attach_tails_to_core\n",
    "print(\"core:\", core_mol)\n",
    "print(\"tails:\", tail_mols)\n",
    "whole_mol = attach_tails_to_core(core_mol[0], tail_mols)\n",
    "print(time_1-time_0)\n",
    "print(\"assembled:\", whole_mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [11:10<00:00,  2.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "generated = []\n",
    "for _ in tqdm(range(2000)):\n",
    "    samples = model.sample(batch_size=1, seq_len=5)\n",
    "    samples.shape\n",
    "    cores_z = samples[:,0,:]\n",
    "    tails_z = samples[0,1:,:]\n",
    "    core_mol = core_model.csample(cores_z, greedy=True)\n",
    "    tail_mols = tail_model.csample(tails_z, greedy=True)\n",
    "    try:\n",
    "        whole_mol = attach_tails_to_core(core_mol[0], tail_mols)\n",
    "        generated.append(whole_mol)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCCC[n+]1ccc(CC(c2ccccc2)c2cccc(C[N+](C)(C)C)c2)cc1\n",
      "CCCCCCCC[n+]1cccc(CCNCCCCC[N+](C)(C)C)c1\n",
      "CCCCCCCCCCC[n+]1ccc(-c2cc[n+](CC)cc2)cc1\n",
      "CCN(C)c1ccc(C(c2ccccc2)c2ccc(N(C)C)cc2)cc1\n",
      "CCCCCC[N+](C)(C)c1ccc(C(c2ccccc2)c2cccc(C[N+](C)(C)C)c2)cc1\n",
      "CCCCCCCCCCCc1ccccc1\n",
      "CCCCCCCCCC[n+]1cccc(-c2ccccc2)c1\n",
      "CCCCCCCCCCCC[N+](C)(C)C\n"
     ]
    }
   ],
   "source": [
    "# Batched Generation\n",
    "\n",
    "# Suppose you want n_samples molecules in a batch, and each sample has (1 core + 4 tails) = 5 total.\n",
    "n_samples = 8  # Just an example\n",
    "seq_len = 5    # 1 core + 4 tails\n",
    "\n",
    "# Sample all at once\n",
    "samples = model.sample(batch_size=n_samples, seq_len=seq_len)\n",
    "# samples.shape -> (n_samples, seq_len, latent_dim)\n",
    "\n",
    "# Separate the first latent vector as core, remaining as tails\n",
    "cores_z = samples[:, 0, :]     # Shape: (n_samples, latent_dim)\n",
    "tails_z = samples[:, 1:, :]    # Shape: (n_samples, seq_len - 1, latent_dim)\n",
    "\n",
    "# Decode *all* cores in one go\n",
    "core_mols = core_model.csample(cores_z, greedy=True)\n",
    "# core_mols is a list of length n_samples (or another iterable structure), each a molecule\n",
    "\n",
    "# Decode *all* tails in one go\n",
    "# Reshape tails_z to flatten out the 2nd dimension so you can pass them in a single batch\n",
    "n_tails = seq_len - 1  # 4 in this example\n",
    "latent_dim = tails_z.shape[-1]\n",
    "tails_z_reshaped = tails_z.reshape(n_samples * n_tails, latent_dim)\n",
    "\n",
    "tail_mols_flat = tail_model.csample(tails_z_reshaped, greedy=True)\n",
    "# tail_mols_flat is length (n_samples * 4)\n",
    "\n",
    "# Regroup the flat tail molecules back into n_samples chunks\n",
    "# Each chunk has 4 tail molecules for that sample\n",
    "tail_mols_grouped = [\n",
    "    tail_mols_flat[i*n_tails : (i+1)*n_tails]\n",
    "    for i in range(n_samples)\n",
    "]\n",
    "\n",
    "for i in range(n_samples):\n",
    "    try:\n",
    "        core_mol = core_mols[i]\n",
    "        tails_for_this_core = tail_mols_grouped[i]  # 4 tails\n",
    "        whole_mol = attach_tails_to_core(core_mol, tails_for_this_core)\n",
    "        print(whole_mol)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating molecule {i}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/generated-20241230.txt\", \"w\") as file:\n",
    "    for line in generated:\n",
    "        file.write(line + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7df1d8b499a47504649464ef14cc540ce0a3f157eaf8a1bd6897e9769f098f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
